# Описание решения
За основу был взят пример из статьи https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/ для игры CartPole-v0
## Исследования
### Архитектура
Наиболее стабильного результата получилось добиться при 3 скрытых слоях по 128-128-64 нейрона. При добавлении большего количества нейронов появлялось зависание и модуль не желал опускаться ниже середины экрана.
___
Функции активации остались без изменения: relu - на скрытых и linear - на выходном. При попытках изменить на другие награда уменьшалась.
### Policy
Экспериментальным путём было выявлено два фаворита: MaxBoltzmannQPolicy() и BoltzmannGumbelQPolicy(). Решено было остановиться на первом кандидате.
### Количество шагов для обучения
Выяснилось, что больше не значит лучше. Изначально обучение происходило на 100.000 шагах, выдавая ~210-220 в среднем за 100 эпизодов. 
Но оказалось, что уже после 80.000 шагов награда падает, так что я остановилась на 70.000 шагов, повысив результат на ~10-15 очков.
***
Также при обучении оказалось полезным увеличить повторение действий. При изменении с 1 на 3 результат увеличился до 250. Но тогда оставалась одна проблема: когда модуль приземляется не точно в центр, то боковой двигатель продолжал работать, из-за чего терялись баллы.
Но изменив параметр до 4, проблема исчезла.
# Результат
Объединив всё выше сказанное, получаем на тестах около 275-280 за 100 эпизодов.

#
P.S. Это индивидуальное решение, так как остальные участники команды Big Mind не принимали участия в данной разработке.
