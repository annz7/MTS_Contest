# Описание решения
За основу был взят пример из статьи https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/ для игры CartPole-v0
## Исследования
### Архитектура
Наиболее стабильного результата получилось добиться при 3 скрытых слоях по 128-128-64 нейрона. При добавлении большего количества нейронов появлялось зависание и модуль не желал опускаться ниже середины экрана.
___
Функции активации остались без изменения: relu - на скрытых и linear - на выходном. При попытках изменить на другие награда уменьшалась.
### Policy
Экспериментальным путём было выявлено два фаворита: MaxBoltzmannQPolicy() и BoltzmannGumbelQPolicy(). Решено было остановиться на первом кандидате.
### Количество шагов для обучения
Выяснилось, что больше не значит лучше. Изначально обучение происходило на 100.000 шагах, выдавая ~210-220 в среднем за 100 эпизодов. 
Но оказалось, что уже после 80.000 шагов награда падает, так что я остановилась на 70.000 шагов, повысив результат на ~10-15 очков.
***
Также при обучении оказалось полезным изменить повторение действий с 1 на 3.
# Результат
Объединив всё выше сказанное, получаем на тестах около 250 за 100 эпизодов. Агент стабильно садится на площадку, не разбиваясь.
Но осталась одна проблема: когда модуль приземляется не точно в центр, то не всегда выключается боковой двигатель, из-за чего теряются баллы.
